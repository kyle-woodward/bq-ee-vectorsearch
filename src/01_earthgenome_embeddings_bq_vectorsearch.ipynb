{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kyle-woodward/bq-ee-vectorsearch/blob/main/src/01_earthgenome_embeddings_bq_vectorsearch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b7046e",
      "metadata": {
        "id": "b5b7046e"
      },
      "source": [
        "## BigQuery ELT of EarthGenome Embeddings for Vector Search\n",
        "\n",
        "*Note: This notebook will create and consume resources on Google Cloud. Though it should be minimal, be mindful of cost and always delete resources after running demos.*\n",
        "\n",
        "In order to run this demo you will need Google Cloud IAM permissions to:\n",
        "* read, write, and create Cloud Storage objects\n",
        "* read, write, and create BigQuery resources\n",
        "\n",
        "Refer to [docs](https://cloud.google.com/iam/docs/roles-overview) for more info if you get a permissions-related error"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "118d2630",
      "metadata": {
        "id": "118d2630"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import geopandas as gpd\n",
        "import subprocess\n",
        "from google.cloud import bigquery\n",
        "import datetime"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6a1b380",
      "metadata": {
        "id": "d6a1b380"
      },
      "source": [
        "### Configure AWS credentials (you'll need an AWS account and a key created)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fa2c3d",
      "metadata": {
        "id": "05fa2c3d"
      },
      "outputs": [],
      "source": [
        "!pip install awscli\n",
        "!aws --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15b2b367",
      "metadata": {
        "id": "15b2b367"
      },
      "outputs": [],
      "source": [
        "!aws configure"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5e2f37",
      "metadata": {
        "id": "5f5e2f37"
      },
      "source": [
        "### Configure Google Cloud Credentials & Resources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1135a87a",
      "metadata": {
        "id": "1135a87a"
      },
      "outputs": [],
      "source": [
        "# change to your GCS settings\n",
        "BUCKET = \"gs://YOUR-BUCKET\" # GC Storage bucket\n",
        "PROJECT_ID = \"YOUR-PROJECT\" # GC project\n",
        "LOCATION = \"YOUR-REGION\" # compute region\n",
        "DATASET_ID = \"YOUR_DATASET\" # BigQuery Dataset\n",
        "TABLE_ID = \"YOUR_TABLE\" # BigQuery Table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gcloud auth login\n",
        "!gcloud config set project {PROJECT_ID}"
      ],
      "metadata": {
        "id": "sNHKLojCl3ME"
      },
      "id": "sNHKLojCl3ME",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6241cf09",
      "metadata": {
        "id": "6241cf09"
      },
      "outputs": [],
      "source": [
        "import google.auth\n",
        "scopes = ['https://www.googleapis.com/auth/cloud-platform']\n",
        "creds, _ = google.auth.default(scopes=scopes, default_scopes=scopes, quota_project_id=PROJECT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befee486",
      "metadata": {
        "id": "befee486"
      },
      "source": [
        "## Downloading Earthgenome Geoparquet's\n",
        "\n",
        "### Earth Genome has hosted it on Source.Coop - let's check how its organized -> [link](https://source.coop/repositories/earthgenome/earthindexembeddings/description)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb23f79c",
      "metadata": {
        "id": "bb23f79c"
      },
      "source": [
        "### In [00_s2_tile_management.ipynb](./00_s2_tile_management.ipynb) we've already aggregated UTM tile IDs to country boundaries\n",
        "\n",
        "#### we'll use that JSON file to help us pull only the EG parquet files we need for a country.."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ef450efd",
      "metadata": {
        "id": "ef450efd"
      },
      "outputs": [],
      "source": [
        "# Read in our country-tile JSON reference\n",
        "!mkdir -p ../esa_grid && wget https://raw.githubusercontent.com/kyle-woodward/bq-ee-vectorsearch/refs/heads/main/esa_grid/adm0_tiles_by_country.json -O ../esa_grid/adm0_tiles_by_country.json\n",
        "tile_dict = json.load(open(\"../esa_grid/adm0_tiles_by_country.json\"))\n",
        "print(tile_dict.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a29fd37b",
      "metadata": {
        "id": "a29fd37b"
      },
      "outputs": [],
      "source": [
        "country = \"Kenya\"\n",
        "tiles = tile_dict[country]\n",
        "tiles.sort()\n",
        "print(f\"{len(tiles)} S2 tiles covering {country}\")\n",
        "for t in tiles:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8555160",
      "metadata": {
        "id": "c8555160"
      },
      "outputs": [],
      "source": [
        "dryrun=False\n",
        "\n",
        "for i,t in enumerate(tiles):\n",
        "    # limit data we're downloading..\n",
        "    if i > 0:\n",
        "        break\n",
        "\n",
        "    suffix = \"2024-01-01_2025-01-01.parquet\"\n",
        "    pattern = f\"s3://earthgenome/earthindexembeddings/2024/{t}_{suffix}\"\n",
        "    cmd = f\"aws s3 cp {pattern} ../embeddings/earthgenome/2024/{t}_{suffix} --endpoint-url=https://data.source.coop\"\n",
        "    if dryrun:\n",
        "        print(cmd)\n",
        "    else:\n",
        "        print(f\"Running {cmd}\")\n",
        "        try:\n",
        "            subprocess.run(cmd, shell=True, capture_output=True, check=True)\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"Error copying {t}: {e}\")\n",
        "            # If the file does not exist, we can skip it\n",
        "            if \"does not exist\" in e.stderr.decode():\n",
        "                print(f\"File {t} does not exist, skipping.\")\n",
        "                continue\n",
        "            else:\n",
        "                raise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "effae846",
      "metadata": {
        "id": "effae846"
      },
      "source": [
        "Look at a geoparquet file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa0240f",
      "metadata": {
        "id": "cfa0240f"
      },
      "outputs": [],
      "source": [
        "# look at one\n",
        "files = os.listdir(\"../embeddings/earthgenome/2024\")\n",
        "print(f\"{len(list(files))} files:\\n {files}\")\n",
        "file = os.path.join(\"../embeddings/earthgenome/2024\", files[0])\n",
        "print(file)\n",
        "df = gpd.read_parquet(file)\n",
        "print(df.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66bddd0b",
      "metadata": {
        "id": "66bddd0b"
      },
      "source": [
        "we'll add a tile column to help us stay organized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90bbd87",
      "metadata": {
        "id": "a90bbd87"
      },
      "outputs": [],
      "source": [
        "# overwrite all files to add tile column\n",
        "for file in files:\n",
        "    file_path = os.path.join(\"../embeddings/earthgenome/2024\", file)\n",
        "    df = gpd.read_parquet(file_path)\n",
        "    df.loc[:,'tile'] = os.path.basename(file).split(\"_\")[0]\n",
        "    df.to_parquet(file_path, index=False)\n",
        "    print(f\"Updated {file} with tile column.\")\n",
        "    break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "987cbb46",
      "metadata": {
        "id": "987cbb46"
      },
      "outputs": [],
      "source": [
        "print(gpd.read_parquet(file_path).head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2a29e15",
      "metadata": {
        "id": "c2a29e15"
      },
      "source": [
        "### Loading Data into BigQuery\n",
        "\n",
        "You'll need a GCS bucket and a BigQuery Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92ef946d",
      "metadata": {
        "id": "92ef946d"
      },
      "outputs": [],
      "source": [
        "# create the storage bucket and BigQuery dataset\n",
        "!gcloud storage buckets create {BUCKET} --location {LOCATION} --project {PROJECT_ID}\n",
        "!bq mk -d --data_location={LOCATION} --project_id {PROJECT_ID} {DATASET_ID}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a090991f",
      "metadata": {
        "id": "a090991f"
      },
      "outputs": [],
      "source": [
        "# upload parquet files to gcs\n",
        "# try gcloud storage sync..\n",
        "gcloud_folder = f\"{BUCKET}/earthgenome/2024\"\n",
        "!gcloud storage rsync ../embeddings/earthgenome/2024 $gcloud_folder \\\n",
        "    --project=$PROJECT_ID"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea6f952e",
      "metadata": {
        "id": "ea6f952e"
      },
      "outputs": [],
      "source": [
        "FULL_TABLE = f\"{PROJECT_ID}:{DATASET_ID}.{TABLE_ID}\"\n",
        "FOLDER = \"earthgenome/2024\"\n",
        "print(FULL_TABLE)\n",
        "for i,file in enumerate(files):\n",
        "    # limit what we're ingesting to BQ\n",
        "    if i > 0:\n",
        "        break\n",
        "    URI = f\"{BUCKET}/{FOLDER}/{file}\"\n",
        "\n",
        "    print(URI)\n",
        "    !bq --location=$LOCATION --project_id=$PROJECT_ID \\\n",
        "            load \\\n",
        "                --source_format=PARQUET \\\n",
        "                $FULL_TABLE \\\n",
        "                $URI"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2012cf5",
      "metadata": {
        "id": "a2012cf5"
      },
      "source": [
        "### Minor transforms of the BQ table\n",
        "\n",
        "we will do a small post-processing query on the loaded embeddings table to get the embedding field converted correctly for vector search..\n",
        "\n",
        "vector search indexing requires the embedding field to be of type `ARRAY<FLOAT>`\n",
        "\n",
        "the load operation turns 'embedding' field into a double-nested STRUCT data type, innermost child containing list of floats..\n",
        "\n",
        "so we have to unpack that list from the nested structure, final data type being `ARRAY<FLOAT64>`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a6004a",
      "metadata": {
        "id": "d2a6004a"
      },
      "outputs": [],
      "source": [
        "query = f\"\"\"\n",
        "SELECT\n",
        "  eg.id,\n",
        "  eg.tile,\n",
        "  ST_GEOGFROMTEXT(grouped.geometry_text) AS geometry,\n",
        "  ARRAY_AGG(e.element) AS embedding\n",
        "FROM\n",
        "  `{PROJECT_ID}`.`{DATASET_ID}`.`{TABLE_ID}` AS eg\n",
        "CROSS JOIN\n",
        "  UNNEST(eg.embedding.list) AS e\n",
        "JOIN (\n",
        "  SELECT id, tile, ST_ASTEXT(geometry) AS geometry_text\n",
        "  FROM `{PROJECT_ID}`.`{DATASET_ID}`.`{TABLE_ID}`\n",
        "  GROUP BY id, tile, geometry_text\n",
        ") AS grouped ON eg.id = grouped.id AND eg.tile = grouped.tile AND ST_ASTEXT(eg.geometry) = grouped.geometry_text\n",
        "GROUP BY eg.id, eg.tile, grouped.geometry_text\n",
        "\"\"\"\n",
        "\n",
        "# Run the query and save the result to a new table\n",
        "result_table = f\"{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}_v1\"\n",
        "job_config = bigquery.QueryJobConfig(destination=result_table)\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "job = client.query(query, job_config=job_config)\n",
        "job.result()  # Wait for the job to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09872f41",
      "metadata": {
        "id": "09872f41"
      },
      "outputs": [],
      "source": [
        "# Check if the result_table exists\n",
        "\n",
        "def table_exists(client, table_id):\n",
        "    try:\n",
        "        client.get_table(table_id)\n",
        "        print(f\"Table {table_id} exists.\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"Table {table_id} does not exist. Error: {e}\")\n",
        "        return False\n",
        "\n",
        "table_exists(client, result_table)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66faa394",
      "metadata": {
        "id": "66faa394"
      },
      "outputs": [],
      "source": [
        "# check the resulting table's schema and data\n",
        "query = f\"SELECT * FROM `{result_table}` LIMIT 10\"\n",
        "query_job = client.query(query)\n",
        "# print schema\n",
        "schema = query_job.result().schema\n",
        "for field in schema:\n",
        "    print(f\"{field.name}: {field.field_type}\")\n",
        "for row in query_job:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f99bcf3",
      "metadata": {
        "id": "1f99bcf3"
      },
      "source": [
        "### Index BQ table to enable Vector Search\n",
        "\n",
        "Following [docs](https://cloud.google.com/bigquery/docs/vector-search#create_a_vector_index) guidance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb4a2621",
      "metadata": {
        "id": "eb4a2621"
      },
      "outputs": [],
      "source": [
        "# test VECTOR SEARCH operations\n",
        "in_table = '.'.join(result_table.split(\".\")[1:])\n",
        "print(f'indexing {in_table} for vector search')\n",
        "query = f\"\"\"\n",
        "CREATE VECTOR INDEX my_index ON {in_table}(embedding)\n",
        "OPTIONS(distance_type='COSINE', index_type='IVF', ivf_options='{{\"num_lists\": 1000}}');\n",
        "\"\"\"\n",
        "\n",
        "# Run the query to create the index\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "job = client.query(query)\n",
        "job.result()  # Wait for the job to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f64252b",
      "metadata": {
        "id": "4f64252b"
      },
      "source": [
        "Create a test target table of 1 record to perform vector search with"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9904455b",
      "metadata": {
        "id": "9904455b"
      },
      "outputs": [],
      "source": [
        "result_table = result_table+\"_test_target\"\n",
        "query = f\"SELECT * FROM {in_table} LIMIT 1\"\n",
        "\n",
        "job_config = bigquery.QueryJobConfig(destination=result_table)\n",
        "job = client.query(query,job_config=job_config)\n",
        "job.result()  # Wait for the job to complete"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d2a465d",
      "metadata": {
        "id": "2d2a465d"
      },
      "source": [
        "Run a [Vector Search](https://cloud.google.com/bigquery/docs/reference/standard-sql/search_functions#vector_search)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e769af4b",
      "metadata": {
        "id": "e769af4b"
      },
      "outputs": [],
      "source": [
        "target_table = '.'.join(result_table.split(\".\")[1:])\n",
        "print(target_table)\n",
        "query = f\"\"\"\n",
        "SELECT query.id AS target_id,\n",
        "  query.tile AS target_tile,\n",
        "  base.id AS base_id,\n",
        "  base.tile AS base_tile,\n",
        "  distance\n",
        "FROM\n",
        "  VECTOR_SEARCH(\n",
        "    TABLE {in_table},\n",
        "    'embedding',\n",
        "    TABLE {target_table},\n",
        "    top_k => 11,\n",
        "    distance_type => 'COSINE',\n",
        "    options => '{{\"fraction_lists_to_search\": 0.005}}')\n",
        "ORDER BY distance\n",
        "LIMIT 10\n",
        "OFFSET 1;\n",
        "\"\"\"\n",
        "\n",
        "# Run the query to create the index\n",
        "client = bigquery.Client(project=PROJECT_ID)\n",
        "search_result_table = f\"{PROJECT_ID}.{DATASET_ID}.vector_search_results_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
        "job_config = bigquery.QueryJobConfig(destination=search_result_table)\n",
        "job = client.query(query,job_config=job_config)\n",
        "job.result()  # Wait for the job to complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "547b3571",
      "metadata": {
        "id": "547b3571"
      },
      "outputs": [],
      "source": [
        "query = f\"SELECT * FROM `{search_result_table}` LIMIT 10\"\n",
        "query_job = client.query(query)\n",
        "# print schema\n",
        "schema = query_job.result().schema\n",
        "for field in schema:\n",
        "    print(f\"{field.name}: {field.field_type}\")\n",
        "for row in query_job:\n",
        "    print(row)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b3d1a6",
      "metadata": {
        "id": "84b3d1a6"
      },
      "source": [
        "### You can take a look at your newly created BQ tables and the vector search results in [BQ studio](https://console.cloud.google.com/bigquery)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "ee",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}